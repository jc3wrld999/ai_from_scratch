{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62530625",
   "metadata": {},
   "source": [
    "## GD vs SGD(Stochastic Gradient Descent)\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "GD(Gradient Descent) 는 시작 지점에서 기울기의 반대 방향으로 하강하면서 손실 함수(loss function)를 최소화하는 지점을 찾기 위한 가장 직관적인 방법입니다. 이처럼 전체 데이터 셋을 가지고 학습하게 되면 안정적이긴 하지만, 계산량과 학습 비용이 많아지게 됩니다.\n",
    "\n",
    "이때 전체 데이터 셋이 아닌, 무작위로 뽑은 데이터들에 대한 Gradient Descent를 진행하고, 이를 반복하며 정확도를 찾아 나가는 것을 SGD(Stochastic Gradient Descent)라고 합니다.\n",
    "\n",
    "이번 실습에서는 동일한 모델 생성 및 학습을 통하여 두 최적화 기법을 비교해보도록 하겠습니다.\n",
    "\n",
    "데이터셋은 IMDB 영화 리뷰 데이터 셋을 사용합니다. 해당 데이터셋은 훈련용 데이터 25,000개와 테스트용 데이터 25,000개로 이루어져 있으며, 레이블은 긍정/부정으로 두 가지입니다. 이때 긍정은 1, 부정은 0으로 표시되어 있습니다. 우리의 목표는 전처리된 영화 리뷰 데이터를 가지고 그 리뷰가 긍정적인지 혹은 부정적인지를 예측하는 것입니다.\n",
    "\n",
    "```python\n",
    "모델 구조 확인과 학습을 위한 함수/라이브러리\n",
    "\n",
    "model.summary()\n",
    ": 생성한 모델의 구조를 확인합니다.\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs = 20, batch_size = 500, validation_data = (test_data, test_labels), verbose = 0)\n",
    ": 모델을 학습시킬 때 검증용 데이터를 설정하는 방법입니다. 이전 장의 실습과는 다르게, 이번에는 모델을 학습시킬 때 검증용 데이터(validation_data)를 설정합니다.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb3befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "# 시각화 함수\n",
    "def Visulaize(histories, key='binary_crossentropy'):\n",
    "    #plt.figure(figsize=(,20))\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "\n",
    "    plt.savefig(\"plot.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9988e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# 데이터를 전처리하는 함수\n",
    "\n",
    "def sequences_shaping(sequences, dimension):\n",
    "    \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0 \n",
    "        \n",
    "    return results\n",
    "\n",
    "'''\n",
    "1. GD를 적용할 모델을 자유롭게 생성합니다.\n",
    "모델을 생성할 때 다음 사항을 참고합니다.\n",
    "\n",
    "입력층의 input_shape 인자를 어떻게 설정해야 할지 생각해보세요.\n",
    "출력층의 노드 수는 1, 활성화 함수는 sigmoid로 설정하세요.\n",
    "히든층은 자유롭게 설계하세요.\n",
    "'''\n",
    "\n",
    "def GD_model(word_num):\n",
    "    \n",
    "    model = tf.keras.Sequential([tf.keras.layers.Dense(32, input_shape = (word_num,), activation='relu'),\n",
    "                                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                                tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "    \n",
    "    # model = tf.keras.models.Sequential([\n",
    "    #     tf.keras.layers.Dense(20, input_dim = 1 ,activation='relu'),\n",
    "    #     tf.keras.layers.Dense(20, activation='relu'),\n",
    "    #     tf.keras.layers.Dense(1)\n",
    "    # ])\n",
    "    return model\n",
    "    \n",
    "'''\n",
    "2. SGD를 적용할 모델을 GD를 적용할 모델과 똑같이 생성합니다. \n",
    "우리는 두 모델을 똑같이 만들되, 학습 시 batch_size를 다르게 해서 각각 GD와 SGD를 적용할 것입니다.\n",
    "'''\n",
    "\n",
    "def SGD_model(word_num):\n",
    "    \n",
    "    model = tf.keras.Sequential([tf.keras.layers.Dense(32, input_shape = (word_num,), activation='relu'),\n",
    "                                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                                tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "                                \n",
    "    \n",
    "    return model\n",
    "\n",
    "'''\n",
    "3. 두 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "---------------------------------------------------------------------------------------------------\n",
    "모델 학습(fit) 과정에서 verbose 변수를 활용하면 모델의 학습 과정 기록을 간단하게 표현할 수 있습니다.\n",
    "verbose = 0: 기록을 나타내지 않음.\n",
    "verbose = 1: 진행 바 형태의 학습 과정 기록을 나타냄.\n",
    "verbose = 2: epoch 당 1줄의 학습 과정 기록을 나타냄.\n",
    "---------------------------------------------------------------------------------------------------\n",
    "두 모델에 대해 손실 함수, 최적화(optimize) 알고리즘, 평가 방법(metrics)은 다음과 같이 설정합니다.\n",
    "\n",
    "손실 함수(loss) : ‘binary_crossentropy’\n",
    "\n",
    "최적화 알고리즘(optimizer) : ‘sgd’\n",
    "\n",
    "평가 방법(metrics): [‘accuracy’, ‘binary_crossentropy’]\n",
    "\n",
    "손실 함수로는 ‘binary crossentropy‘를 사용합니다. binary crossentropy는 두 확률 분포 간의 차이를 측정하는 손실 함수로써,값이 작을수록 모델의 예측 결과가 좋다는 뜻\n",
    "batch_size 설정은 스켈레톤 코드의 주석 설명을 참고\n",
    "---------------------------------------------------------------------------------------------------\n",
    "   Step01. GD 함수와 SGD 함수를 이용해 \n",
    "           두 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 두 모델의 손실 함수, 최적화 알고리즘, \n",
    "           평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 두 모델을 각각 학습시킵니다. \n",
    "           검증용 데이터도 설정해주세요.\n",
    "           'epochs'는 20으로 설정합니다.\n",
    "   \n",
    "           GD를 적용할 경우 학습 시 \n",
    "           전체 데이터 셋(full-batch)을\n",
    "           사용하므로 'batch_size'를 \n",
    "           전체 데이터 개수로 설정합니다. \n",
    "           \n",
    "           SGD를 적용할 경우 학습 시 \n",
    "           미니 배치(mini-batch)를 사용하므로\n",
    "           'batch_size'를 전체 데이터 개수보다 \n",
    "           작은 수로 설정합니다. \n",
    "           \n",
    "           여기선 500으로 설정하겠습니다.\n",
    "   \n",
    "   Step05. 학습된 두 모델을 테스트하고 \n",
    "           binary crossentropy 값을 출력합니다. \n",
    "           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "def main():\n",
    "    \n",
    "    word_num = 100\n",
    "    data_num = 25000\n",
    "    \n",
    "    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.\n",
    "    \n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)\n",
    "    print(f'len(train_data[0]) = {len(train_data[0])}')\n",
    "    print(f'train_data[0] = {train_data[0]}')\n",
    "    \n",
    "    train_data = sequences_shaping(train_data, dimension = word_num)\n",
    "    test_data = sequences_shaping(test_data, dimension = word_num)\n",
    "    \n",
    "    print(f'len(train_data[0]) = {len(train_data[0])}')\n",
    "    print(f'train_data[0] = {train_data[0]}')\n",
    "    print(f'train_labels[0] = {train_labels[0]}')\n",
    "    \n",
    "    gd_model = GD_model(word_num)   # GD를 사용할 모델입니다.\n",
    "    sgd_model = SGD_model(word_num)  # SGD를 사용할 모델입니다.\n",
    "    \n",
    "    gd_model.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy', 'binary_crossentropy'])\n",
    "    sgd_model.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy', 'binary_crossentropy'])\n",
    "    \n",
    "    gd_model.summary()\n",
    "    sgd_model.summary()\n",
    "    \n",
    "    gd_history = gd_model.fit(train_data, train_labels, epochs = 20, batch_size = data_num, validation_data = (test_data, test_labels), verbose = 0)\n",
    "    print('\\n')\n",
    "    sgd_history = sgd_model.fit(train_data, train_labels, epochs = 20, batch_size = data_num, validation_data = (test_data, test_labels), verbose = 0)\n",
    "    \n",
    "    scores_gd = gd_history.history['val_binary_crossentropy'][-1]\n",
    "    scores_sgd = sgd_history.history['val_binary_crossentropy'][-1]\n",
    "    \n",
    "    print('\\nscores_gd: ', scores_gd)\n",
    "    print('scores_sgd: ', scores_sgd)\n",
    "    \n",
    "    Visulaize([('GD', gd_history),('SGD', sgd_history)])\n",
    "    \n",
    "    return gd_history, sgd_history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
