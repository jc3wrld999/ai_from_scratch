{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c0e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score, accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 22)\n",
    "\n",
    "\n",
    "def execute_machine_learning_system(switch):\n",
    "    def load_dataset():\n",
    "        bank = pd.read_csv('data_bank.csv', sep=';')\n",
    "        bank.replace('unknown', np.nan, inplace=True)\n",
    "        bank['y'].replace({'no':0, 'yes':1}, inplace=True)\n",
    "        return bank\n",
    "    \n",
    "    def check_missing_values(df):\n",
    "        df_null = df.isnull().sum()\n",
    "        print(\">> Bank Marketing 데이터의 변수별 결측치 비율은 다음과 같습니다.\")\n",
    "        for col, val in df_null.items():\n",
    "            print(\"{} : {:.2f}%\".format(col, val/df.shape[0]*100))\n",
    "    \n",
    "    def get_null_columns(df):\n",
    "        null_sum = df.isnull().sum()\n",
    "        null_cols = df.columns[np.where(null_sum>0)].values\n",
    "        print(\">> Missing Value가 있는 컬럼 : \", null_cols)\n",
    "        return null_cols\n",
    "    \n",
    "    def set_dtypes(df, num_cols, cat_cols):\n",
    "        for col in df.columns:\n",
    "            if col in cat_cols:\n",
    "                df[col] = df[col].astype(np.object)\n",
    "            elif col in num_cols:\n",
    "                df[col] = df[col].astype(np.float)\n",
    "                \n",
    "    def label_encoding_categorical(df):\n",
    "        for col, dtype in zip(df.columns, df.dtypes):\n",
    "            if dtype == np.object:\n",
    "                df[[col]] = df[[col]].apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    def create_dataset(df):\n",
    "        idx = df.dropna(axis=0).index\n",
    "        test_idx = np.random.RandomState(68).choice(idx, 3952)\n",
    "        train_idx = ~bank.index.isin(test_idx)\n",
    "        x = df.drop('y', axis=1).copy()\n",
    "        y = df[['y']]\n",
    "        \n",
    "        x_train = x.loc[train_idx]\n",
    "        x_test = x.loc[test_idx]\n",
    "        y_train = y.loc[train_idx].values.ravel()\n",
    "        y_test = y.loc[test_idx].values.ravel()\n",
    "\n",
    "        print('>> Create Dataset : 모델 학습을 위해 최종 데이터를 생성합니다.')\n",
    "        return  x, x_train, x_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    #컨트롤러\n",
    "    def handling_missing_value(strategy1=None, strategy2=None):\n",
    "        if strategy1 is None and strategy2 is None:\n",
    "            print(\"[INFO] Handling Missing Value : 아무런 정보를 입력하지 않아 실행되지 않습니다.\")\n",
    "            return None\n",
    "        \n",
    "        if strategy1 is None: #전략1: 결측치가 있는 row는 제거. 몇개까지 row안에 결측치가 있는지 허용할거냐\n",
    "            pass\n",
    "        else:\n",
    "            remove_missing_value(bank, threshold=strategy1)\n",
    "            \n",
    "        if strategy2 is None:\n",
    "            pass\n",
    "        else:\n",
    "            if strategy2:\n",
    "                fill_missing_value(bank)\n",
    "            else:\n",
    "                remove_missing_value(bank, threshold=0)\n",
    "        sleep(1.2)\n",
    "\n",
    "    def add_age_categorical(strategy=True):\n",
    "        if strategy:\n",
    "            generate_feature_age(bank)\n",
    "            sleep(1.2)\n",
    "        else:\n",
    "            return None        \n",
    "            \n",
    "    def add_marketing_info(strategy=True):\n",
    "        if strategy:\n",
    "            print('>> Add Marketing Info Features : 이전 마케팅 정보 Feature를 사용합니다.')\n",
    "            sleep(1.2)\n",
    "        else:\n",
    "            remove_past_marketing_info(bank)\n",
    "    \n",
    "    def add_social_economic_info(strategy=True):\n",
    "        if strategy:\n",
    "            print('>> Add Social & Economic Info Features : 사회, 경제적인 정보 Feature를 사용합니다.')\n",
    "            sleep(1.2)\n",
    "        else:\n",
    "            remove_social_economic_info(bank)  \n",
    "    \n",
    "    def transform_pdays_to_categorical(strategy=True):\n",
    "        if strategy:\n",
    "            pdays_to_categorical(bank)\n",
    "            sleep(1.2)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def transform_duration_to_log_scale(strategy=True):\n",
    "        if strategy:\n",
    "            transform_duration(bank)\n",
    "            sleep(1.2)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def show_result_roc_curve(y_test, y_proba, y_predict, model='choosed_model'):\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        plot_roc_curve(fpr, tpr, model)\n",
    "        plt.savefig('roc_curve.svg', format='svg')\n",
    "        elice_utils.send_image(\"roc_curve.svg\")\n",
    "\n",
    "    def show_result_confusion_matrix(y_test, y_predict):\n",
    "        cnf_matrix_ = confusion_matrix(y_true=y_test.squeeze(), y_pred=y_predict)\n",
    "        plot_confusion_matrix(cnf_matrix_, classes=['Yes','No'], title='Confusion matrix')\n",
    "        plt.savefig('confusion_matrix.svg', format='svg')\n",
    "        elice_utils.send_image(\"confusion_matrix.svg\")\n",
    "\n",
    "    \n",
    "    #필요함수\n",
    "    ### Handling Missing Value\n",
    "    def remove_missing_value(df, threshold=0):\n",
    "        if type(threshold) != int:\n",
    "            print(\"[ERROR] threshold 에는 숫자(int)를 입력해주세요!\")\n",
    "            return None\n",
    "        if threshold < 0:\n",
    "            print(\"[ERROR] 0보다 작은 값을 입력할 수 없습니다.\")\n",
    "            return None\n",
    "        if threshold > 5:\n",
    "            threshold = 5\n",
    "        thresh = df.shape[1] - threshold\n",
    "        df.dropna(axis=0, thresh=thresh, inplace=True)\n",
    "        print(\">> Handling Missing Value : 결측치가 '제거'되었습니다.\")\n",
    "    \n",
    "    def fill_missing_value(df):\n",
    "        # num_cols, cat_cols, null_cols가 선행적으로 assign 되어 있어야 함\n",
    "        for col in null_cols:\n",
    "            if col in cat_cols:\n",
    "                _fill_most_frequent(df, col)\n",
    "            elif col in num_cols:\n",
    "                _fill_median(df, col)\n",
    "            else:\n",
    "                pass\n",
    "        print(\">> Handling Missing Value : 결측치가 '처리'되었습니다.\")\n",
    "    \n",
    "    def _fill_most_frequent(df, col):\n",
    "        most_frequent = df[col].value_counts().index[0] \n",
    "        df[col].fillna(most_frequent, inplace=True)\n",
    "        \n",
    "    def _fill_median(df, col):\n",
    "        median = df[col].median()\n",
    "        df[col].fillna(median, inplace=True)\n",
    "    \n",
    "    \n",
    "    ### Feature Generation\n",
    "    def generate_feature_age(df):\n",
    "        df['age_cat'] = pd.cut(df['age'], bins=[0, 25, 45, 60, np.inf], labels=[1, 2, 3, 4])\n",
    "        cat_cols.append('age_cat')\n",
    "        print('>> Generate Feature \"연령군\" : age 변수로부터 새로운 feature를 생성하였습니다.')\n",
    "    \n",
    "    \n",
    "    ### Feature Selection\n",
    "    def remove_past_marketing_info(df):\n",
    "        marketing_cols = ['contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays']\n",
    "        df.drop(marketing_cols, axis=1, inplace=True)\n",
    "        for col in marketing_cols:\n",
    "            if col in num_cols:\n",
    "                num_cols.remove(col)\n",
    "            elif col in cat_cols:\n",
    "                cat_cols.remove(col)\n",
    "    \n",
    "    def remove_social_economic_info(df):\n",
    "        social_economic_cols = ['emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "        df.drop(social_economic_cols, axis=1, inplace=True)\n",
    "        for col in social_economic_cols:\n",
    "            if col in num_cols:\n",
    "                num_cols.remove(col)\n",
    "            elif col in cat_cols:\n",
    "                cat_cols.remove(col)\n",
    "    \n",
    "    def choose_model_algorithm(model_selected='logistic_regression'):\n",
    "        model_list =  ['linear_regression','logistic_regression', 'decision_tree',\n",
    "\t\t\t\t\t   'knn', 'ridge_regression', 'k-means', 'lasso_regression',\n",
    "\t\t\t\t\t   'naive_bayes', 'neural_network', 'random_forest'] \n",
    "        if model_selected not in model_list:\n",
    "            s = '''모델 알고리즘을 아래 예시에 있는 것을 정확하게 입력했는지 확인해주세요.\\n{}'''.format(model_list)\n",
    "            raise ValueError(s)\n",
    "        if model_selected == 'logistic_regression':\n",
    "            model = LogisticRegression(fit_intercept=False, random_state=1234)\n",
    "            \n",
    "        elif model_selected == 'decision_tree':\n",
    "            model = DecisionTreeClassifier(criterion='gini', random_state=1234)\n",
    "            \n",
    "        elif model_selected == 'knn':\n",
    "            model = KNeighborsClassifier(n_neighbors = 10, weights='uniform', p=2, metric='euclidean')\n",
    "            \n",
    "        elif model_selected == 'naive_bayes':\n",
    "            model =  GaussianNB()\n",
    "            \n",
    "        elif model_selected == 'support_vector_machine':\n",
    "            model = SVC(kernel = 'sigmoid', random_state=1234)\n",
    "            \n",
    "        elif model_selected == 'neural_network':\n",
    "            model = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(8, 4), random_state=1234)\n",
    "            \n",
    "        elif model_selected == 'random_forest':\n",
    "            model = RandomForestClassifier(n_estimators = 20, random_state=1234)\n",
    "\n",
    "        elif model_selected == 'linear_regression':\n",
    "            s = '''선택한 모델 알고리즘 `{}`은 회귀(regression) 문제에 사용됩니다.\\n분류 문제를 위한 모델 알고리즘을 선택해주세요.'''.format(model_selected)\n",
    "            raise ValueError(s)\n",
    "\n",
    "        elif model_selected == 'ridge_regression':\n",
    "            s = '''선택한 모델 알고리즘 `{}`은 회귀(regression) 문제에 사용됩니다.\\n분류 문제를 위한 모델 알고리즘을 선택해주세요.'''.format(model_selected)\n",
    "            raise ValueError(s)\n",
    "\n",
    "        elif model_selected == 'lasso_regression':\n",
    "            s = '''선택한 모델 알고리즘 `{}`은 회귀(regression) 문제에 사용됩니다.\\n분류 문제를 위한 모델 알고리즘을 선택해주세요.'''.format(model_selected)\n",
    "            raise ValueError(s)\n",
    "\n",
    "        elif model_selected == 'k-means':\n",
    "            s = '''선택한 모델 알고리즘 `{}`은 군집화(clustering) 문제에 사용됩니다.\\n분류 문제를 위한 모델 알고리즘을 선택해주세요.'''.format(model_selected)\n",
    "            raise ValueError(s)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    ### Feature Transform\n",
    "    def pdays_to_categorical(df):\n",
    "        try:\n",
    "            pdays = df['pdays']\n",
    "    \n",
    "            df.loc[(pdays>=0) & (pdays<5), 'pdays'] = 1\n",
    "            df.loc[(pdays>=5) & (pdays<10), 'pdays'] = 2\n",
    "            df.loc[(pdays>=10) & (pdays<15), 'pdays'] = 3\n",
    "            df.loc[(pdays>=15) & (pdays<20), 'pdays'] = 4\n",
    "            df.loc[(pdays>=20) & (pdays<25), 'pdays'] = 5\n",
    "            df.loc[(pdays>=25) & (pdays<999), 'pdays'] = 6\n",
    "            df.loc[(pdays==999), 'pdays'] = 7\n",
    "            num_cols.remove('pdays')\n",
    "            cat_cols.append('pdays')\n",
    "            print('>> Transform Feature : Feature \"pdays\"가 Categorical Feature로 변환되었습니다.')\n",
    "        except KeyError:\n",
    "            print('[INFO] Transform \"pdays\" To Categorical : 이전 마케팅 정보를 사용하지 않으므로 실행되지 않습니다.')\n",
    "    \n",
    "    def remove_pdays(df):\n",
    "        df.drop('pdays', axis=1, inplace=True)\n",
    "        num_cols.remove('pdays')\n",
    "        \n",
    "    @np.vectorize\n",
    "    def log_ignore(x):\n",
    "        if x != 0:\n",
    "            return math.log(x)\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def transform_duration(df):\n",
    "        try:\n",
    "            df['duration'] = df['duration'].map(log_ignore)\n",
    "            print('>> Transform Feature : Feature \"duration\"이 Log Scale로 변환되었습니다.')\n",
    "        except:\n",
    "            print('[INFO] Transform Feature \"duration\" : 이전 마케팅 정보를 사용하지 않으므로 실행되지 않습니다.')\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    def cleansing_categorical(indices_categorical_columns):\n",
    "        categorical_pipline =  Pipeline(steps=[\n",
    "                        ('select', FunctionTransformer(lambda data: data[:, indices_categorical_columns])),\n",
    "                        ('onehot', OneHotEncoder(sparse=False))\n",
    "                    ])\n",
    "        return categorical_pipline\n",
    "    \n",
    "    \n",
    "    def cleansing_numeric(indices_numeric_columns, how=None):\n",
    "        if how is None:\n",
    "            print('>> Feature Normalization : feature의 scale을 그대로 사용합니다.')\n",
    "            numeric_pipeline = Pipeline(steps=[\n",
    "                            ('select', FunctionTransformer(lambda data: data[:, indices_numeric_columns])),\n",
    "                        ])\n",
    "            return numeric_pipeline\n",
    "        elif how == 'standard':\n",
    "            print('>> Feature Normalization : Feature를 Standardization으로 scaling합니다.')\n",
    "            numeric_pipeline = Pipeline(steps=[\n",
    "                            ('select', FunctionTransformer(lambda data: data[:, indices_numeric_columns])),\n",
    "                            ('scale', StandardScaler())\n",
    "                        ])\n",
    "            return numeric_pipeline\n",
    "        elif how == 'minmax':\n",
    "            print('>> Feature Normalization : Feature를 Min-Max 0과 1사이로 scaling합니다.')\n",
    "            numeric_pipeline = Pipeline(steps=[\n",
    "                            ('select', FunctionTransformer(lambda data: data[:, indices_numeric_columns])),\n",
    "                            ('scale', MinMaxScaler())\n",
    "                        ])\n",
    "            return numeric_pipeline\n",
    "        else:\n",
    "            s = '[ERROR] Feature Normalization을 위한 방법은 None, \"standard\", \"minmax\"로 정확하게 입력하셔야 합니다.'\n",
    "            raise ValueError(s)\n",
    "            \n",
    "    \n",
    "    def create_estimator(df, model, scaling=None):\n",
    "        indices_categorical_columns = df.dtypes == np.object\n",
    "        indices_numeric_columns = df.dtypes != np.object\n",
    "        if indices_categorical_columns.sum() != 0 and indices_numeric_columns.sum() != 0:\n",
    "            estimator = Pipeline(steps=[\n",
    "                ('cleansing', FeatureUnion(transformer_list=[\n",
    "                    ('categorical', cleansing_categorical(indices_categorical_columns)),\n",
    "                    ('numeric', cleansing_numeric(indices_numeric_columns, how=scaling))\n",
    "                ])),\n",
    "                ('modeling', model)\n",
    "            ])\n",
    "        elif indices_categorical_columns.sum() !=0 and indices_numeric_columns.sum() == 0:\n",
    "            estimator = Pipeline(steps=[\n",
    "                ('cleansing', FeatureUnion(transformer_list=[\n",
    "                    ('categorical', cleansing_categorical(indices_categorical_columns))\n",
    "                ])),\n",
    "                ('modeling', model)\n",
    "            ])\n",
    "        elif indices_categorical_columns.sum() ==0 and indices_numeric_columns.sum() != 0:\n",
    "            estimator = Pipeline(steps=[\n",
    "                ('cleansing', FeatureUnion(transformer_list=[\n",
    "                    ('numeric', cleansing_numeric(indices_numeric_columns, how=scaling))\n",
    "                ])),\n",
    "                ('modeling', model)\n",
    "            ])\n",
    "        else:\n",
    "            return None\n",
    "        return estimator\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    def plot_roc_curve(fpr, tpr, model, color=None) :\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "    \n",
    "        accuracy = round(accuracy_score(y_test.squeeze(), y_predict),8)*100\n",
    "        model = model + '\\n(AUC = {:0.3f}, Accuracy={:.2f}%)'.format(auc(fpr, tpr), accuracy)\n",
    "        \n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.plot(fpr, tpr, label=model, color=color, linewidth=4)\n",
    "        plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "        plt.axis([0,1,0,1])\n",
    "        plt.xlabel('FPR', fontsize=22)\n",
    "        plt.ylabel('TPR', fontsize=22)\n",
    "        plt.title('ROC curve', fontsize=35)\n",
    "        plt.legend(loc=\"lower right\", fontsize=19)\n",
    "    \n",
    "    def plot_confusion_matrix(cm, classes,\n",
    "                              normalize=False,\n",
    "                              title='Confusion matrix',\n",
    "                              cmap=plt.cm.Blues):\n",
    "        \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        \"\"\"\n",
    "        plt.style.use('seaborn')\n",
    "        plt.figure(figsize=(7,7))\n",
    "        import itertools\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum()\n",
    "            title = 'Normalized Rate of Confusion Matrix'\n",
    "    \n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title, fontsize=35)\n",
    "        #plt.colorbar( fraction=0.046, pad=0.04, use_gridspec=True)\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45, fontsize=18)\n",
    "        plt.yticks(tick_marks, classes, fontsize=18)\n",
    "    \n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            \n",
    "            plt.text(j, i, format(cm[~i, ~j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\",fontsize=30)\n",
    "    \n",
    "        plt.ylabel('Actual label', fontsize=21)\n",
    "        plt.xlabel('Predicted label', fontsize=21)\n",
    "        plt.tight_layout()\n",
    "        plt.grid(False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #전략 선택 부분\n",
    "    switch_handling_missing_value_1 = switch['handling_missing_value_1']\n",
    "    switch_handling_missing_value_2 = switch['handling_missing_value_2']\n",
    "    switch_add_age_categorical = switch['add_age_categorical']\n",
    "    switch_add_marketing_info = switch['add_marketing_info']\n",
    "    switch_add_social_economic_info = switch['add_social_economic_info']\n",
    "    switch_transform_pdays_to_categorical = switch['transform_pdays_to_categorical']\n",
    "    switch_transform_duration_to_log_scale = switch['transform_duration_to_log_scale']\n",
    "    switch_feature_normalization = switch['feature_normalization']\n",
    "    model_selection = switch['model_selection']\n",
    "    \n",
    "    #준비\n",
    "    bank = load_dataset()\n",
    "    null_cols = get_null_columns(bank)\n",
    "    num_cols = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "    cat_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "    \n",
    "    #전략 실행 부분\n",
    "    ## Data Preprocessing\n",
    "    handling_missing_value(strategy1=switch_handling_missing_value_1,\n",
    "                           strategy2=switch_handling_missing_value_2)\n",
    "    ### Feature Generation\n",
    "    add_age_categorical(strategy=switch_add_age_categorical)\n",
    "    add_marketing_info(strategy=switch_add_marketing_info)\n",
    "    add_social_economic_info(strategy=switch_add_social_economic_info)\n",
    "    ### Feature Transform\n",
    "    transform_pdays_to_categorical(strategy=switch_transform_pdays_to_categorical)\n",
    "    transform_duration_to_log_scale(strategy=switch_transform_duration_to_log_scale)\n",
    "    \n",
    "    # 최종데이터 셋 준비\n",
    "    set_dtypes(bank, num_cols, cat_cols)\n",
    "    label_encoding_categorical(bank)\n",
    "    set_dtypes(bank, num_cols, cat_cols)\n",
    "    x, x_train, x_test, y_train, y_test = create_dataset(bank)\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=3952, shuffle=True, stratify=y,random_state=13)\n",
    "    del bank\n",
    "    \n",
    "    # 모델학습\n",
    "    model = choose_model_algorithm(model_selection)\n",
    "    estimator = create_estimator(x, model, scaling=switch_feature_normalization)\n",
    "    estimator.fit(x_train, y_train)\n",
    "    \n",
    "    # 모델결과\n",
    "    #model_score = estimator.score(x_test, y_test)\n",
    "    y_predict = estimator.predict(x_test)\n",
    "    y_proba = estimator.predict_proba(x_test)[:, 1]\n",
    "    print('...')\n",
    "    sleep(1.9)\n",
    "    print('>>> 모델을 학습하는 중입니다.')\n",
    "    sleep(1.9)\n",
    "    print('>>> 모델 학습이 완료되었습니다. 학습된 모델을 사용해 3,952명에 대한 예측을 진행합니다.')\n",
    "    sleep(3)\n",
    "\n",
    "    # 시각화\n",
    "    print('=============================')\n",
    "    print('========= 예측 결과 =========')\n",
    "    print('=============================')\n",
    "    show_result_confusion_matrix(y_test, y_predict)\n",
    "    sleep(2.2)\n",
    "    show_result_roc_curve(y_test, y_proba, y_predict, model_selection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7aede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Missing Value가 있는 컬럼 :  ['job' 'marital' 'education' 'default' 'housing' 'loan']\n",
      ">> Handling Missing Value : 결측치가 '제거'되었습니다.\n",
      ">> Handling Missing Value : 결측치가 '처리'되었습니다.\n",
      ">> Generate Feature \"연령군\" : age 변수로부터 새로운 feature를 생성하였습니다.\n",
      ">> Add Marketing Info Features : 이전 마케팅 정보 Feature를 사용합니다.\n",
      ">> Add Social & Economic Info Features : 사회, 경제적인 정보 Feature를 사용합니다.\n",
      ">> Transform Feature : Feature \"duration\"이 Log Scale로 변환되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:50: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  df[col] = df[col].astype(np.float)\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:48: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  df[col] = df[col].astype(np.object)\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:54: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if dtype == np.object:\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:54: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if dtype == np.object:\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:54: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if dtype == np.object:\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:54: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if dtype == np.object:\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:54: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if dtype == np.object:\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:54: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if dtype == np.object:\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:54: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if dtype == np.object:\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:54: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if dtype == np.object:\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:54: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if dtype == np.object:\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:54: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if dtype == np.object:\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:54: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if dtype == np.object:\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:50: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  df[col] = df[col].astype(np.float)\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:48: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  df[col] = df[col].astype(np.object)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Create Dataset : 모델 학습을 위해 최종 데이터를 생성합니다.\n",
      ">> Feature Normalization : Feature를 Standardization으로 scaling합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:323: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  indices_categorical_columns = df.dtypes == np.object\n",
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_8488\\4165935010.py:324: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  indices_numeric_columns = df.dtypes != np.object\n"
     ]
    },
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), age               False\njob                True\nmarital            True\neducation          True\ndefault            True\nhousing            True\nloan               True\ncontact            True\nmonth              True\nday_of_week        True\nduration          False\ncampaign          False\npdays             False\nprevious          False\npoutcome           True\nemp.var.rate      False\ncons.price.idx    False\ncons.conf.idx     False\neuribor3m         False\nnr.employed       False\nage_cat            True\ndtype: bool)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\_libs\\index.pyx:144\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(slice(None, None, None), age               False\njob                True\nmarital            True\neducation          True\ndefault            True\nhousing            True\nloan               True\ncontact            True\nmonth              True\nday_of_week        True\nduration          False\ncampaign          False\npdays             False\nprevious          False\npoutcome           True\nemp.var.rate      False\ncons.price.idx    False\ncons.conf.idx     False\neuribor3m         False\nnr.employed       False\nage_cat            True\ndtype: bool)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m switch\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m \t\u001b[43mpredictive_model_for_target_marketing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [2], line 21\u001b[0m, in \u001b[0;36mpredictive_model_for_target_marketing\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m아래 준비된 9개의 스위치의 값을 변경하면서\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m타겟마케팅을 했을 때 응할 것 같은 고객을 예측하기 위한 머신러닝 모델을 만들어보세요.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m※주의!  \"#\" 뒤에 있는 값만 입력해주세요!\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      9\u001b[0m switch \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhandling_missing_value_1\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhandling_missing_value_2\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_selection\u001b[39m\u001b[38;5;124m'\u001b[39m :  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecision_tree\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     19\u001b[0m }\n\u001b[1;32m---> 21\u001b[0m \u001b[43mexecute_machine_learning_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mswitch\u001b[49m\u001b[43m)\u001b[49m\t\t\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m switch\n",
      "Cell \u001b[1;32mIn [1], line 445\u001b[0m, in \u001b[0;36mexecute_machine_learning_system\u001b[1;34m(switch)\u001b[0m\n\u001b[0;32m    443\u001b[0m model \u001b[38;5;241m=\u001b[39m choose_model_algorithm(model_selection)\n\u001b[0;32m    444\u001b[0m estimator \u001b[38;5;241m=\u001b[39m create_estimator(x, model, scaling\u001b[38;5;241m=\u001b[39mswitch_feature_normalization)\n\u001b[1;32m--> 445\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# 모델결과\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m#model_score = estimator.score(x_test, y_test)\u001b[39;00m\n\u001b[0;32m    449\u001b[0m y_predict \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\pipeline.py:378\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    377\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 378\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\pipeline.py:336\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    334\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    337\u001b[0m     cloned_transformer,\n\u001b[0;32m    338\u001b[0m     X,\n\u001b[0;32m    339\u001b[0m     y,\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    341\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    342\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    344\u001b[0m )\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\pipeline.py:870\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 870\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\pipeline.py:1154\u001b[0m, in \u001b[0;36mFeatureUnion.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit all transformers, transform the data and concatenate results.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;124;03m        sum of `n_components` (output dimension) over transformers.\u001b[39;00m\n\u001b[0;32m   1153\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1154\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[0;32m   1156\u001b[0m         \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\pipeline.py:1176\u001b[0m, in \u001b[0;36mFeatureUnion._parallel_func\u001b[1;34m(self, X, y, fit_params, func)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformer_weights()\n\u001b[0;32m   1174\u001b[0m transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter())\n\u001b[1;32m-> 1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFeatureUnion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\pipeline.py:870\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 870\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\pipeline.py:414\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03mFits all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;124;03m    Transformed samples.\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    413\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 414\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[0;32m    416\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\pipeline.py:336\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    334\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    337\u001b[0m     cloned_transformer,\n\u001b[0;32m    338\u001b[0m     X,\n\u001b[0;32m    339\u001b[0m     y,\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    341\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    342\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    344\u001b[0m )\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\pipeline.py:870\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 870\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\base.py:870\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:212\u001b[0m, in \u001b[0;36mFunctionTransformer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"Transform X using the forward function.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m    Transformed input.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    211\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:292\u001b[0m, in \u001b[0;36mFunctionTransformer._transform\u001b[1;34m(self, X, func, kw_args)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     func \u001b[38;5;241m=\u001b[39m _identity\n\u001b[1;32m--> 292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kw_args \u001b[38;5;28;01mif\u001b[39;00m kw_args \u001b[38;5;28;01melse\u001b[39;00m {}))\n",
      "Cell \u001b[1;32mIn [1], line 290\u001b[0m, in \u001b[0;36mexecute_machine_learning_system.<locals>.cleansing_categorical.<locals>.<lambda>\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcleansing_categorical\u001b[39m(indices_categorical_columns):\n\u001b[0;32m    289\u001b[0m     categorical_pipline \u001b[38;5;241m=\u001b[39m  Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m--> 290\u001b[0m                     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselect\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m data: \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_categorical_columns\u001b[49m\u001b[43m]\u001b[49m)),\n\u001b[0;32m    291\u001b[0m                     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monehot\u001b[39m\u001b[38;5;124m'\u001b[39m, OneHotEncoder(sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    292\u001b[0m                 ])\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m categorical_pipline\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\core\\frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3804\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3806\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3810\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3805\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m         \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m-> 3810\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;66;03m# GH#42269\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5966\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5962\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   5963\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   5964\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   5965\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> 5966\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (slice(None, None, None), age               False\njob                True\nmarital            True\neducation          True\ndefault            True\nhousing            True\nloan               True\ncontact            True\nmonth              True\nday_of_week        True\nduration          False\ncampaign          False\npdays             False\nprevious          False\npoutcome           True\nemp.var.rate      False\ncons.price.idx    False\ncons.conf.idx     False\neuribor3m         False\nnr.employed       False\nage_cat            True\ndtype: bool)"
     ]
    }
   ],
   "source": [
    "\n",
    "def predictive_model_for_target_marketing():\n",
    "    \n",
    "    '''\n",
    "    아래 준비된 9개의 스위치의 값을 변경하면서\n",
    "    타겟마케팅을 했을 때 응할 것 같은 고객을 예측하기 위한 머신러닝 모델을 만들어보세요.\n",
    "    \n",
    "    ※주의!  \"#\" 뒤에 있는 값만 입력해주세요!\n",
    "    '''\n",
    "    switch = {\n",
    "        'handling_missing_value_1' : 0,\n",
    "        'handling_missing_value_2' : True, \n",
    "        'add_age_categorical' : True,  \n",
    "        'add_marketing_info' : True, \n",
    "        'add_social_economic_info' : True, \n",
    "        'transform_pdays_to_categorical' : False,\t\t\t \n",
    "        'transform_duration_to_log_scale' : True, \n",
    "        'feature_normalization' :  'standard', \n",
    "        'model_selection' :  'decision_tree'\n",
    "    }\n",
    "    \n",
    "    execute_machine_learning_system(switch)\t\t\n",
    "    \n",
    "    return switch\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tpredictive_model_for_target_marketing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757043bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_value(tp, fn, fp, tn):\n",
    "    tp_value = 7380\n",
    "    fn_value = 0\n",
    "    fp_value = -2620\n",
    "    tn_value = 0\t\n",
    "    \n",
    "    total = tp + fn + fp + tn\n",
    "    input_true = tp + fn\n",
    "    input_false = fp + tn\n",
    "    if total != 3952:\n",
    "        s = '''예측 대상인 3,952명 보다 많거나 적은 수가 입력되었습니다.\\nConfusion Matrix에 적힌 수를 정확하게 입력해주시기 바랍니다.'''\n",
    "        raise ValueError('{}'.format(s))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ev = (tp/total * tp_value) + (fn/total * fn_value) + (fp/total * fp_value) + (tn/total * tn_value)\n",
    "    revenue = (tp * tp_value) + (fn * fn_value) + (fp * fp_value) + (tn * tn_value)\n",
    "    print(\"개발한 모델의 기대손익은 {:,.0f}원 입니다.\".format(ev))\n",
    "    print(\"개발한 모델을 사용하여 타겟마케팅을 진행했을 때 예상 수익은 {:,.0f}원 입니다.\".format(revenue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d91382c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개발한 모델의 기대손익은 293원 입니다.\n",
      "개발한 모델을 사용하여 타겟마케팅을 진행했을 때 예상 수익은 1,157,120원 입니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_expected_value():\n",
    "    \n",
    "    '''\n",
    "    [실습] 모델의 기대손익과 예상 수익 평가하기\n",
    "    \n",
    "    1) 이전 실습에서 Confusion Matrix의 결과값을 가져오세요\n",
    "        --------------------------------------\n",
    "        |                 |                  |\n",
    "        |  true_positive  |  false_negative  |\n",
    "        |                 |                  |\n",
    "        --------------------------------------\n",
    "        |                 |                  |\n",
    "        |  false_positive |  true_negative   |\n",
    "        |                 |                  |\n",
    "        --------------------------------------\n",
    "    \n",
    "    2) 아래에 각 값을 입력하세요 (총 3,952명을 대상으로 합니다)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # 이전 실습에서 Confusion Matrix의 결과값을 가져와 입력하세요\n",
    "    true_positive = 253\n",
    "    false_negative = 247\n",
    "    false_positive = 271\n",
    "    true_negative = 3181\t\n",
    "    \n",
    "    \n",
    "    # 모델의 기대손익(expected value)과\n",
    "    # 모델을 사용했을 때의 예상 수익(revenue)을 확인합니다\n",
    "    expected_value(\n",
    "                    true_positive, false_negative,\n",
    "                    false_positive, true_negative\n",
    "                    )\n",
    "    \n",
    "    \n",
    "    return true_positive,false_negative,false_positive,true_negative\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_expected_value()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
