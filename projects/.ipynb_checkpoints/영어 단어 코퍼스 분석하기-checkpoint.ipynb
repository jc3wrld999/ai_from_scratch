{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bdfd984",
   "metadata": {},
   "source": [
    "## 영어단어의 특성과 단어의 사용 빈도수\n",
    "\n",
    "두 번째 프로젝트에서는 약 1억 개의 영어 단어로 구성된 [British National Corpus (BNC)](https://en.wikipedia.org/wiki/British_National_Corpus) 텍스트 코퍼스(단어모음)를 다루어봅니다. \n",
    "\n",
    "`words.txt` 파일에 저장된 단어 10,000개의 빈도수를 그래프로 시각화한 후 선형회귀법을 적용해보겠습니다. 주석을 읽고 실행 결과가 출력되도록 `do_linear_regression()` 함수를 완성하세요.\n",
    "\n",
    "## 과제\n",
    "\n",
    "1. 주석과 함께 코드를 이해하고 각 함수의 리턴값을 확인하세요. \n",
    "\n",
    "2. `do_linear_regression()` 함수를 작성하세요 (44번째 줄).\n",
    "\n",
    "3. 실행 버튼을 누르고 출력되는 차트를 확인하세요. \n",
    "\n",
    "4. `main()` 함수에서 21, 22번째 줄의 주석을 해제하세요. 실행 버튼을 누르고 새롭게 출력되는 차트를 확인하세요.\n",
    "\n",
    "5. 출력된 그래프와 아래 **[실행 결과]**를 비교해본 후 제출 버튼을 눌러보세요. \n",
    "\n",
    "**[실행 결과]**\n",
    "![result]({{ result.png }})\n",
    "\n",
    "## 부가 설명\n",
    "\n",
    "[지프의 법칙](http://scienceon.hani.co.kr/?document_srl=34047)에 따르면 자연어 코퍼스의 단어들을 순서대로 나열할 때 사용 빈도가 가장 높은 단어는 두 번째 단어 보다 빈도가 약 두 배 높으며, 세 번째 단어보다는 빈도가 세 배 높습니다. \n",
    "\n",
    "즉, 거듭제곱 분포를 보이기 때문에 주석을 해제한 후 `log(X), log(Y)`를 적용하면 더욱 정확하게 선형회귀 분석을 할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "717777ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import elice_utils\n",
    "\n",
    "def main():\n",
    "    words = read_data()\n",
    "\n",
    "\n",
    "\n",
    "    words = sorted(words, key=lambda k: k[1] , reverse=True) # words.txt 단어를 빈도수 순으로 정렬합니다.\n",
    "    \n",
    "    # 정수로 표현된 단어를 X축 리스트에, 각 단어의 빈도수를 Y축 리스트에 저장합니다.  \n",
    "    X = list(range(1, len(words)+1))\n",
    "    Y = [x[1] for x in words]\n",
    "    \n",
    "    # X, Y 리스트를 array로 변환합니다. \n",
    "    X, Y = np.array(X).reshape(-1,1), np.array(Y)\n",
    "    \n",
    "    # X, Y의 각 원소 값에 log()를 적용합니다.\n",
    "    X, Y = np.log(X), np.log(Y)\n",
    "    \n",
    "    # 기울기와 절편을 구한 후 그래프와 차트를 출력합니다. \n",
    "    slope, intercept = do_linear_regression(X, Y)\n",
    "    draw_chart(X, Y, slope, intercept)\n",
    "    \n",
    "    return slope, intercept\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# read_data() - words.txt에 저장된 단어와 해당 단어의 빈도수를 리스트형으로 변환합니다.\n",
    "def read_data():\n",
    "    # words.txt 에서 단어들를 읽어,\n",
    "    # [[단어1, 빈도수], [단어2, 빈도수] ... ]형으로 변환해 리턴합니다.\n",
    "    words=[]\n",
    "    words_txt = open(\"words.txt\")\n",
    "    for line in words_txt:\n",
    "        l = line.split(',')\n",
    "        words.append([l[0],int(l[1])])\n",
    "    return words\n",
    "\n",
    "\n",
    "# do_linear_regression() - 임포트한 sklearn 패키지의 함수를 이용해 그래프의 기울기와 절편을 구합니다.\n",
    "def do_linear_regression(X, Y):\n",
    "\n",
    "\n",
    "\n",
    "    # do_linear_regression() 함수를 작성하세요. \n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X,Y)\n",
    "    slope = lr.coef_\n",
    "    intercept = lr.intercept_\n",
    "    \n",
    "    return (slope, intercept)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# draw_chart() - matplotlib을 이용해 차트를 설정합니다.\n",
    "def draw_chart(X, Y, slope, intercept):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.scatter(X, Y)\n",
    "\n",
    "    # 차트의 X, Y축 범위와 그래프를 설정합니다.\n",
    "    min_X = min(X)\n",
    "    max_X = max(X)\n",
    "    min_Y = min_X * slope + intercept\n",
    "    max_Y = max_X * slope + intercept\n",
    "    plt.plot([min_X, max_X], [min_Y, max_Y], \n",
    "             color='red',\n",
    "             linestyle='--',\n",
    "\n",
    "\n",
    "\n",
    "             linewidth=3.0)\n",
    "    \n",
    "    # 기울과와 절편을 이용해 그래프를 차트에 입력합니다.\n",
    "    ax.text(min_X, min_Y + 0.1, r'$y = %.2lfx + %.2lf$' % (slope, intercept), fontsize=15)\n",
    "    \n",
    "    plt.savefig('chart.png')\n",
    "    elice_utils.send_image('chart.png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfaccac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['z', '7'],\n",
       "       ['z', '1'],\n",
       "       ['z', '2'],\n",
       "       ...,\n",
       "       ['&', '6'],\n",
       "       ['&', '1'],\n",
       "       ['&', '6']], dtype='<U1')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# words = pd.read_table('./words.txt', sep=',')\n",
    "np.loadtxt('words.txt', delimiter=',', dtype=('str', 'int'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
