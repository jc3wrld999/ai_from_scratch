{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88a19b5",
   "metadata": {},
   "source": [
    "## 네이버 영화평 감정분석\n",
    "\n",
    "실제 영화 리뷰를 이용해 나이브 베이즈 분류기를 학습시키고, 입력 받은 영화평이 긍정 또는 부정적일 확률을 구하는 감정 분석(Sentiment Analysis)을 해보겠습니다. 데이터는 네이버 개발자 Lucy Park님의 Naver Sentiment Movie Corpus v1.0를 사용합니다. 네이버 영화의 140자 영화평을 모은 것으로 총 100,000개의 부정 리뷰, 100,000개의 긍정 리뷰로 구성되어 있습니다.\n",
    "```\n",
    "id\tdocument\tlabel\n",
    "9251303\t와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런게 진짜 영화지\t1\n",
    "10067386\t안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.\t1\n",
    "2190435\t사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화\t1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1b5f46",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0xec in position 26: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 148\u001b[0m\n\u001b[0;32m    145\u001b[0m     elice_utils\u001b[38;5;241m.\u001b[39msend_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage.svg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [8], line 18\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# read_data는 두 개의 string으로 이루어진 array를 리턴합니다.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# array의 첫 번째 string은 모든 부정적 리뷰가 space로 나뉘어 합쳐진 string,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# array의 두 번째 string은 모든 긍정적 리뷰가 space로 나뉘어 합쳐진 string입니다.\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     training_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     testing_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m어설픈 연기들로 몰입이 전혀 안되네요\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m     prob_pair \u001b[38;5;241m=\u001b[39m naive_bayes(training_sentences, testing_sentence)\n",
      "Cell \u001b[1;32mIn [8], line 55\u001b[0m, in \u001b[0;36mread_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m training_sentences \u001b[38;5;241m=\u001b[39m [[], []]\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m fp:\n\u001b[0;32m     57\u001b[0m         splitted \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0xec in position 26: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy\n",
    "import matplotlib as mpl\n",
    "mpl.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import elice_utils\n",
    "import re\n",
    "import math\n",
    "\n",
    "special_chars_remover = re.compile(\"[^\\w'|_]\")\n",
    "def remove_special_characters(sentence):\n",
    "    return special_chars_remover.sub(' ', sentence)\n",
    "\n",
    "def main():\n",
    "    # read_data는 두 개의 string으로 이루어진 array를 리턴합니다.\n",
    "    # array의 첫 번째 string은 모든 부정적 리뷰가 space로 나뉘어 합쳐진 string,\n",
    "    # array의 두 번째 string은 모든 긍정적 리뷰가 space로 나뉘어 합쳐진 string입니다.\n",
    "    training_sentences = read_data()\n",
    "\n",
    "\n",
    "\n",
    "    testing_sentence = \"어설픈 연기들로 몰입이 전혀 안되네요\"\n",
    "    \n",
    "    prob_pair = naive_bayes(training_sentences, testing_sentence)\n",
    "    # 시각화 코드입니다.\n",
    "    plot_title = testing_sentence\n",
    "    if len(plot_title) > 50: plot_title = plot_title[:50] + \"...\"\n",
    "    visualize_boxplot(plot_title,\n",
    "                  list(prob_pair),\n",
    "                  ['Negative', 'Positive'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def naive_bayes(training_sentences, testing_sentence):\n",
    "    log_prob_negative = calculate_doc_prob(training_sentences[0], testing_sentence, 0.1) + math.log(0.5)\n",
    "    log_prob_positive = calculate_doc_prob(training_sentences[1], testing_sentence, 0.1) + math.log(0.5)\n",
    "\n",
    "\n",
    "\n",
    "    prob_pair = normalize_log_prob(log_prob_negative, log_prob_positive)\n",
    "    \n",
    "    return prob_pair\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_data():\n",
    "\n",
    "\n",
    "\n",
    "    training_sentences = [[], []]\n",
    "    \n",
    "    with open('ratings.txt') as fp:\n",
    "        next(fp)\n",
    "        for line in fp:\n",
    "            splitted = line.split(\"\\t\")\n",
    "            document = splitted[1]\n",
    "            label = int(splitted[2])\n",
    "            training_sentences[label].append(document)\n",
    "    \n",
    "    return [' '.join(training_sentences[0]), ' '.join(training_sentences[1])]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_log_prob(prob1, prob2):\n",
    "    maxprob = max(prob1, prob2)\n",
    "\n",
    "    prob1 -= maxprob\n",
    "    prob2 -= maxprob\n",
    "    prob1 = math.exp(prob1)\n",
    "    prob2 = math.exp(prob2)\n",
    "\n",
    "    normalize_constant = 1.0 / float(prob1 + prob2)\n",
    "    prob1 *= normalize_constant\n",
    "    prob2 *= normalize_constant\n",
    "\n",
    "    return (prob1, prob2)\n",
    "\n",
    "def calculate_doc_prob(training_sentence, testing_sentence, alpha):\n",
    "    logprob = 0\n",
    "    training_model = create_BOW(training_sentence)\n",
    "    testing_model = create_BOW(testing_sentence)\n",
    "    num_tokens = 0\n",
    "    for w in training_model:\n",
    "        num_tokens += training_model[w]\n",
    "    for word in testing_model:\n",
    "        cnt = testing_model[word]\n",
    "        if (word in training_model):\n",
    "            cnt_train = training_model[word]\n",
    "            logprob += cnt * (math.log(cnt_train / num_tokens))\n",
    "        else:\n",
    "            logprob += cnt *(math.log(alpha / num_tokens))\n",
    "    return logprob\n",
    "\n",
    "def create_BOW(sentence):\n",
    "\n",
    "\n",
    "\n",
    "    bow = {}\n",
    "    \n",
    "    # preprocessing\n",
    "    sentence = sentence.lower()\n",
    "    sentence = remove_special_characters(sentence)\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if len(token) < 1:\n",
    "            continue\n",
    "        bow.setdefault(token, 0)\n",
    "        bow[token] += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return bow\n",
    "\n",
    "def visualize_boxplot(title, values, labels):\n",
    "    width = .35\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(title)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ind = numpy.arange(len(values))\n",
    "    rects = ax.bar(ind, values, width)\n",
    "    ax.bar(ind, values, width=width)\n",
    "    ax.set_xticks(ind + width/2)\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def autolabel(rects):\n",
    "        # attach some text labels\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.text(rect.get_x()+rect.get_width()/2., height + 0.01, '%.2lf%%' % (height * 100), ha='center', va='bottom')\n",
    "\n",
    "    autolabel(rects)\n",
    "\n",
    "    plt.savefig(\"image.svg\", format=\"svg\")\n",
    "    elice_utils.send_image(\"image.svg\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc3bd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csjty\\AppData\\Local\\Temp\\ipykernel_27092\\495972172.py:1: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  data = pd.read_csv('ratings.txt', 'r')\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 131, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mratings.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m data\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 131, saw 2\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('ratings.txt', 'r')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b93e36",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [Lucy Park님의 Naver Sentiment Movie Corpus v1.0](https://github.com/e9t/nsmc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
